"""
CSV Data Loader for Augmented NeoDataset

Loads user story data from augmented NeoDataset CSV files generated by
the weak supervision pipeline (scripts/augment_neodataset.py).
"""
import pandas as pd
import os
from typing import List, Optional

from config import NEODATASET_PATH, NEODATASET_HIGH_CONF_PATH
from src.models.story import Story


class CSVDataLoader:
    """
    CSV implementation of data loader for augmented NeoDataset.
    This is the primary data source for SprintGuard.
    """
    
    def __init__(self, csv_path: str = NEODATASET_PATH, high_conf_only: bool = False):
        """
        Initialize CSV data loader.
        
        Args:
            csv_path: Path to augmented NeoDataset CSV
            high_conf_only: If True, use only high-confidence subset
        """
        if high_conf_only:
            csv_path = NEODATASET_HIGH_CONF_PATH
        
        self.csv_path = csv_path
        self._df = None
        self._load_data()
    
    def _load_data(self):
        """Load data from CSV file"""
        print(f"\n[DATA LOADER] Loading augmented NeoDataset...")
        print(f"  Path: {self.csv_path}")
        
        if not os.path.exists(self.csv_path):
            raise FileNotFoundError(
                f"\n{'='*70}\n"
                f"ERROR: Augmented NeoDataset not found at:\n"
                f"  {self.csv_path}\n\n"
                f"Please run the augmentation pipeline first:\n"
                f"  1. Install ML dependencies: pip install -r requirements-ml.txt\n"
                f"  2. Run augmentation: python scripts/augment_neodataset.py\n"
                f"{'='*70}\n"
            )
        
        try:
            self._df = pd.read_csv(self.csv_path)
            print(f"  ✓ Loaded {len(self._df)} stories")
            print(f"  ✓ Columns: {len(self._df.columns)}")
            
            # Validate schema
            print(f"\n[VALIDATION] Checking data schema...")
            required_cols = ['id', 'description', 'risk_label']
            missing_cols = [col for col in required_cols if col not in self._df.columns]
            if missing_cols:
                print(f"  ⚠ Warning: Missing columns: {missing_cols}")
            else:
                print(f"  ✓ Required columns present")
            
            # Check risk_label values
            if 'risk_label' in self._df.columns:
                unique_labels = self._df['risk_label'].unique()
                label_counts = self._df['risk_label'].value_counts()
                
                print(f"\n[LABELS] Risk label distribution:")
                for label in sorted(unique_labels):
                    count = label_counts.get(label, 0)
                    print(f"  {label}: {count} ({count/len(self._df)*100:.1f}%)")
                
                # Warn about unexpected labels
                expected_binary = {'SAFE', 'RISK'}
                expected_3class = {'Low', 'Medium', 'High'}
                
                if set(unique_labels).issubset(expected_binary):
                    print(f"  ℹ Using binary labels (SAFE/RISK)")
                elif set(unique_labels).issubset(expected_3class):
                    print(f"  ℹ Using 3-class labels (Low/Medium/High)")
                else:
                    print(f"  ⚠ Warning: Unexpected risk_label values: {unique_labels}")
            
            # Check for story_points
            if 'story_points' in self._df.columns:
                has_points = self._df['story_points'].notna().sum()
                print(f"\n[STORY POINTS] {has_points}/{len(self._df)} stories have story_points")
            
            print(f"\n✓ Data loaded successfully: {os.path.basename(self.csv_path)}")
            
        except Exception as e:
            print(f"\n✗ ERROR: Failed to load data: {e}")
            raise
    
    def _row_to_story(self, row) -> Story:
        """Convert CSV row (pandas Series) to Story object"""
        return Story(
            id=int(row.get('id', row.name)),  # Use index if no id column
            description=row.get('description', ''),
            estimated_points=int(row.get('story_points', 0)) if pd.notna(row.get('story_points')) else None,
            actual_points=int(row.get('story_points', 0)) if pd.notna(row.get('story_points')) else None,
            days_to_complete=None,  # Not in NeoDataset
            caused_spillover=None,  # Not in NeoDataset
            risk_level=row.get('risk_label', 'Unknown'),  # From augmentation
            epic=None,
            reporter=None
        )
    
    def get_all_stories(self) -> List[Story]:
        """Retrieve all historical stories"""
        return [self._row_to_story(row) for _, row in self._df.iterrows()]
    
    def get_story_by_id(self, story_id: int) -> Optional[Story]:
        """Retrieve a specific story by ID"""
        matching = self._df[self._df['id'] == story_id]
        if matching.empty:
            return None
        return self._row_to_story(matching.iloc[0])
    
    def get_stories_by_risk_level(self, risk_level: str) -> List[Story]:
        """Retrieve stories filtered by risk level"""
        # Map risk_level to risk_label in NeoDataset
        # risk_level can be "Low", "Medium", "High"
        # risk_label in NeoDataset is "SAFE" or "RISK"
        
        if risk_level.lower() in ['low', 'safe']:
            filtered = self._df[self._df['risk_label'] == 'SAFE']
        elif risk_level.lower() in ['high', 'risk']:
            filtered = self._df[self._df['risk_label'] == 'RISK']
        else:  # Medium - return subset of RISK stories
            filtered = self._df[self._df['risk_label'] == 'RISK']
        
        return [self._row_to_story(row) for _, row in filtered.iterrows()]
    
    def get_story_count(self) -> int:
        """Get total number of stories"""
        return len(self._df)

