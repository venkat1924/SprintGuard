This was generated By Cursor based on all the research

# DistilBERT-XGBoost Risk Prediction Model

## Architecture Overview

Hybrid neuro-symbolic architecture:

- **Neural**: DistilBERT embeddings (768-dim CLS token)
- **Symbolic**: Readability + Ambiguity + Risk Lexicons (~15 features)
- **Classifier**: XGBoost (histogram method) with probability calibration
- **Explainability**: TreeSHAP with template-based narratives
- **Output**: 3-class risk (Low/Medium/High) + confidence + explanation + similar stories

## Implementation Steps

### 1. Symbolic Feature Extraction

**File**: `src/ml/feature_extractors.py`

Implement `SymbolicFeatureExtractor` class with these methods:

**A. Readability (Cognitive Load)**

- Flesch Reading Ease: `textstat.flesch_reading_ease()` (< 30 = High Risk)
- Gunning Fog Index: `textstat.gunning_fog()` (> 16 = High Risk)
- Lexical Density: content_words / total_words

**B. Ambiguity (Verification Risk)**

- Weak Modal Density: Count "might|could|should|may|ought" / sentence_count (> 0.15 threshold)
- Vague Quantifiers: Binary flag for "fast|easy|robust|user-friendly|seamless|efficient|many|few|several|TBD"
- Passive Voice Ratio: `spaCy` POS tagging (> 0.20 threshold)

**C. Risk Lexicons**

- SATD Keywords: Count "hack|fixme|todo|workaround|temporary|ugly|hardcoded"
- Security Terms: Count "auth|token|jwt|encrypt|pii|gdpr|injection|xss|secret"
- Complexity Terms: Count "legacy|mainframe|migration|api|oauth|middleware"

**Output**: 15-dimensional vector (3 readability + 3 ambiguity + 3 lexicons + metadata)

**Excluded from PoC**: Sentiment analysis (research recommends Senti4SD; too specialized for PoC).

---

### 2. DistilBERT Embedding Extraction

**File**: `src/ml/bert_embedder.py`

Implement `BertEmbedder` class:

**Configuration**:

- Model: `distilbert-base-uncased`
- Tokenizer: `max_length=128` (user stories are short; avoid wasted computation)
- Extraction: **CLS token** from `last_hidden_state[:, 0, :]`
  - *Research Note*: Mean pooling recommended, but CLS is standard for classification and simpler for PoC
- Normalization: L2-normalize output (enables Dot Product similarity)

**Optimization**:

- PyTorch Dynamic Quantization (INT8): `torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)`
  - Expected: 2-3x speedup on CPU, <1% accuracy loss
  - *Decision*: Skip ONNX Runtime (simpler deployment for PoC)

**Caching**:

- `functools.lru_cache` decorator (maxsize=1000) keyed on hash(title + description)
- Rationale: Planning sessions review same stories multiple times

**Output**: 768-dimensional normalized embedding

---

### 3. Training Pipeline

**File**: `src/ml/train_risk_model.py`

**Data Preparation**:

1. Load `data/neodataset_augmented_high_confidence.csv`
2. Filter: Keep stories with `risk_confidence > 0.75`
3. Stratified split by `idproject`: 60% train, 20% val, 20% test

**Feature Fusion (Early Fusion - Concatenation)**:

```
X_combined = [Symbolic_15 | Embedding_768] → 783 dimensions
```

- *Research Note*: PCA to 32-64 dims recommended but skipped for PoC (XGBoost handles high-dim adequately)
- Symbolic features: Z-score standardization
- Embeddings: Already L2-normalized, use raw

**XGBoost Configuration** (Research-Backed):

```python
params = {
    'objective': 'multi:softprob',
    'num_class': 3,
    'tree_method': 'hist',
    'max_bin': 64,              # Reduce embedding noise
    'max_depth': 5,             # Shallow trees (4-6 range)
    'min_child_weight': 7,      # Prevent overfitting on outliers (5-10 range)
    'colsample_bytree': 0.4,    # Critical: Force consideration of symbolic features
    'colsample_bynode': 0.6,    # Additional randomization
    'subsample': 0.7,           # Row sampling
    'reg_alpha': 0.5,           # L1 regularization (sparsity)
    'eta': 0.05,                # Learning rate
    'eval_metric': 'mlogloss'
}
```

**Feature Weights** (Enforce Symbolic Importance):

- Symbolic features: weight = 2.0
- Embedding dims: weight = 1.0
- Rationale: Prevents 768 embedding dims from drowning out 15 symbolic features

**Class Imbalance Handling**:

- Compute `sample_weight = class_weight.compute_sample_weight('balanced', y)`
- Pass to XGBoost via `DMatrix(weight=sample_weight)`

**Hyperparameter Tuning** (Optional for PoC):

- If time permits, run Optuna for 50 trials on {max_depth, min_child_weight, alpha, colsample_bytree}
- Otherwise, use baseline values above

**Model Artifacts**:

- `models/xgboost_risk_model.json`
- `models/feature_scaler.pkl` (StandardScaler for symbolic features)
- `models/risk_lexicons.json` (keyword lists)
- `models/feature_names.json` (mapping for SHAP)

---

### 4. Probability Calibration

**File**: `src/ml/calibration.py`

**Why**: Raw XGBoost probabilities are not well-calibrated for cost-sensitive decisions.

**Method**: Isotonic Regression (sklearn)

- Research recommends Dirichlet Calibration for multi-class, but Isotonic is simpler and sufficient for PoC
- Fit on validation set: `CalibratedClassifierCV(xgb_model, method='isotonic', cv='prefit')`

**Output**: `models/calibrated_model.pkl`

---

### 5. Cost-Sensitive Thresholds

**File**: `src/ml/threshold_optimizer.py`

**Cost Matrix** (False Negative for High Risk is most expensive):

```
         Pred: Low  Med  High
True Low:    0     1     3
True Med:    2     0     2
True High:  50     10    0
```

**Decision Rule**:

For calibrated probabilities `[P_Low, P_Med, P_High]`, predict class that minimizes expected cost:

```
Risk_Level = argmin_k Σ P(j) * Cost[k, j]
```

**Threshold Scope**: Global for PoC

- *Research Note*: Project-specific thresholds recommended, but requires per-team calibration (complexity)
- *Future Work*: Implement adaptive thresholds per `idproject`

**Output**: `models/cost_matrix.json`

---

### 6. Inference & Explainability

**File**: `src/ml/risk_predictor.py`

Implement `RiskPredictor` class:

**Prediction Pipeline**:

1. Extract symbolic features (15-dim)
2. Extract & cache DistilBERT embedding (768-dim)
3. Concatenate & scale
4. XGBoost forward pass (calibrated probabilities)
5. Apply cost-sensitive decision rule → Risk Level

**Explainability (TreeSHAP)**:

```python
explainer = shap.TreeExplainer(xgb_model)
shap_values = explainer.shap_values(X_combined)  # Per-class SHAP values
```

**Narrative Generation** (Template-Based):

- Extract top 3 features by absolute SHAP value
- Map feature names to human-readable strings:
  - `gunning_fog > 16` → "Description is too complex (requires graduate-level reading)"
  - `weak_modal_density > 0.15` → "High ambiguity: 8 uncertain words (might, could, should)"
  - `satd_keywords > 0` → "Contains technical debt markers (TODO, hack)"
  - `embedding_dim_X` → "Semantic complexity detected in description"

*Research Note*: LLM-generated narratives (GPT/Llama) recommended for richer explanations, but template-based sufficient for PoC and avoids API dependencies.

**Caching Explanations**:

- Cache `(story_hash → explanation_text)` in memory (dict)
- Rationale: SHAP computation is ~50-100ms; caching makes repeated queries instant

**Latency Target**: < 1 second total

- DistilBERT (quantized): ~30-50ms
- Symbolic features: ~10ms
- XGBoost inference: <1ms
- SHAP computation: ~50ms (cached after first call)
- **Total**: ~100ms (well under budget)

---

### 7. Similar Story Retrieval

**File**: `src/ml/similarity_retriever.py`

Implement `SimilarityRetriever` class:

**Strategy**: Reuse DistilBERT embeddings (no separate model)

- *Research Note*: `sentence-transformers/all-mpnet-base-v2` + FAISS recommended for production, but adds dependencies
- *Decision*: Simple brute-force Dot Product is <5ms for <50k stories

**Algorithm**:

1. Normalize query embedding (L2)
2. Compute `scores = query_embedding @ historical_embeddings.T` (vectorized Dot Product)
3. Return top-k indices
4. Fetch story metadata (title, risk_label, story_points)

**k-NN Configuration**:

- k = 5 (research: 3-5 optimal for cognitive load)
- No re-ranking or filtering for PoC (research suggests risk-aware scoring, but adds complexity)

**Output**: List of `{story_id, title, risk_level, similarity_score}`

---

### 8. Integration with MLRiskAssessor

**File**: `src/analyzers/ml_risk_assessor.py` (modify existing)

**Update `__init__`**:

```python
self.predictor = RiskPredictor.load(model_path)
self.retriever = SimilarityRetriever(historical_stories)
```

**Update `assess()` method**:

```python
def assess(self, description: str) -> RiskResult:
    # Predict risk
    risk_level, confidence, shap_explanation = self.predictor.predict(description)
    
    # Retrieve similar stories
    similar = self.retriever.find_similar(description, k=5)
    
    return RiskResult(
        risk_level=risk_level,
        confidence=confidence,
        explanation=shap_explanation,
        similar_stories=[s['story_id'] for s in similar]
    )
```

---

### 9. Dependencies

**Update**: `requirements.txt`

Add:

```
transformers==4.35.0
torch==2.1.0
xgboost==2.0.2
shap==0.43.0
textstat==0.7.3
spacy==3.7.2
```

Download spaCy model: `python -m spacy download en_core_web_sm`

---

### 10. Validation & Testing

**File**: `tests/test_ml_risk_model.py`

Test coverage:

1. **Symbolic Features**: Assert "This might be easy" triggers modal flag
2. **Embedding**: Verify output shape (768,) and L2 norm ≈ 1.0
3. **End-to-End**: Feed sample story → verify RiskResult structure
4. **Latency**: Benchmark 100 predictions, assert p95 < 1s
5. **SHAP**: Verify explanation contains feature names
6. **Similarity**: Assert similar stories have same risk class

---

## Research Integration Summary

| Research Topic | Recommendation | PoC Decision | Rationale |

|----------------|----------------|--------------|-----------|

| **Embedding Type** | Mean Pooling | CLS Token | Standard for classification, simpler |

| **Dimensionality Reduction** | PCA to 32-64 | Skip | XGBoost handles high-dim; reduces PoC complexity |

| **Fusion Strategy** | Late Fusion | Early Fusion (Concat) | Simpler pipeline; adequate for PoC |

| **Feature Weights** | 2x for tabular | **Include** | Critical for balancing 768 vs 15 features |

| **XGBoost Hyperparams** | Full spec | **Include** | Essential for model quality |

| **Calibration** | Dirichlet | Isotonic Regression | Simpler, sklearn-native |

| **Thresholds** | Project-specific | Global with cost matrix | Reduces complexity; upgradeable later |

| **Quantization** | ONNX Runtime | PyTorch Dynamic | Simpler deployment |

| **XGBoost Optimization** | Treelite compilation | Native XGBoost | Avoid C compilation complexity |

| **Explainability** | FastSHAP + TCAV | TreeSHAP + Templates | Sufficient for PoC |

| **Narratives** | LLM-generated | Template-based | No external API dependencies |

| **Retrieval Model** | MPNet + FAISS | DistilBERT + Brute Force | Reuse embeddings; <5ms is fast enough |

| **Sentiment Analysis** | Senti4SD | Skip | Too specialized for PoC |

| **Drift Detection** | ADWIN | Skip | Post-deployment concern |

---

## Expected Outcomes

- **Accuracy**: >85% F1-score on validation set (research baseline: 88%)
- **Latency**: <200ms typical, <1s p95 (research target: <1s)
- **Interpretability**: Human-readable explanations citing specific features
- **Robustness**: Handles edge cases (empty descriptions, special chars)