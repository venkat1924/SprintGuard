# NeoDataset Augmentation Implementation

## âœ… IMPLEMENTATION COMPLETE

All code for augmenting the NeoDataset with risk labels using Weak Supervision has been successfully implemented.

---

## ðŸ“ Files Created

### Core ML Pipeline Modules (`src/ml/`)
```
src/ml/
â”œâ”€â”€ __init__.py                      (2 lines)
â”œâ”€â”€ neodataset_loader.py            (67 lines)  - Dataset loading & preprocessing
â”œâ”€â”€ labeling_functions.py           (348 lines) - 18 research-backed LFs
â”œâ”€â”€ weak_supervision_pipeline.py    (140 lines) - Snorkel aggregation
â””â”€â”€ cleanlab_pipeline.py            (156 lines) - Noise detection & filtering
```

**Total: 713 lines of research-backed ML code**

### Orchestration Scripts (`scripts/`)
```
scripts/
â”œâ”€â”€ explore_neodataset.py           (49 lines)  - EDA & preprocessing check
â””â”€â”€ augment_neodataset.py           (90 lines)  - Full pipeline orchestrator
```

### Documentation (`docs/`)
```
docs/
â””â”€â”€ AUGMENTATION_STATUS.md          (450 lines) - Complete methodology guide
```

---

## ðŸš€ Quick Start

### Step 1: Install Dependencies
```bash
cd /home/jovyan/SprintGuard
pip install snorkel==0.9.9 cleanlab==2.6.0 datasets==2.14.0
```

### Step 2: Run Augmentation
```bash
python scripts/augment_neodataset.py
```

This will:
1. Download NeoDataset (~20K user stories) from HuggingFace
2. Apply 18 research-backed labeling functions
3. Train Snorkel label model to aggregate votes
4. Use Cleanlab to detect and remove noisy labels
5. Generate `data/neodataset_augmented.csv` with risk labels

**Expected Runtime:** 15-30 minutes (depending on hardware)

---

## ðŸ“Š What You Get

### Output Files
- `data/neodataset_augmented.csv` - Full labeled dataset
- `data/neodataset_augmented_high_confidence.csv` - High-confidence subset (>75% confidence)
- `data/neodataset/neodataset_snorkel_labels.csv` - Intermediate Snorkel output

### Label Schema
Each story receives:
- `risk_label`: "RISK" or "SAFE"
- `risk_label_binary`: 1 (RISK) or 0 (SAFE)
- `risk_confidence`: 0.0-1.0 (higher = more confident)
- `risk_prob_risk`: Probability of RISK
- `risk_prob_safe`: Probability of SAFE

### Expected Metrics
- **Label Health Score**: > 0.60 (indicates good label quality)
- **Total Stories**: ~20,000
- **High-Confidence Stories**: ~10,000-15,000
- **Label Distribution**: Roughly 30-70% split between RISK/SAFE

---

## ðŸ”¬ Scientific Methodology

### Risk Definition
**Risk** = Likelihood of spillover, scope creep, or significant underestimation

A story is labeled **RISK** if it exhibits characteristics that historically correlate with:
- Work not completed in sprint (spillover)
- Requirements expanding during development (scope creep)
- Actual effort significantly exceeding estimate

### Weak Supervision Architecture

#### 1. Labeling Functions (18 total)
**Lexical (ISO 29148 Ambiguity Indicators):**
- Vague adjectives: "user-friendly", "easy", "fast"
- Loopholes: "etc", "and so on", "including but not limited to"
- Uncertain quantifiers: "some", "several", "many"
- Weak verbs: "handle", "support", "manage"
- Temporal idealism: "ideally", "normally", "instantly"

**Metadata (Story Point Analysis):**
- High complexity: â‰¥8 points (Cone of Uncertainty)
- Low complexity: â‰¤2 points (low absolute variance)
- Fibonacci anomalies: non-standard point values

**Structural (Syntactic Completeness):**
- Missing acceptance criteria or list structure
- Dependency/blocking keywords
- Very short (<15 words) or very long (>200 words) descriptions

**Domain-Specific:**
- Integration: API, third-party, webhooks
- Legacy: refactor, technical debt
- Security: vulnerability, encryption
- Performance: optimization, scalability
- Bug fixes: typically safe (negative signal)
- Documentation: typically safe (negative signal)

#### 2. Snorkel Data Programming
- Applies all 18 LFs to generate label matrix
- Trains generative model to learn LF accuracies and correlations
- Aggregates votes into probabilistic labels
- **No ground truth required**

#### 3. Cleanlab Confident Learning
- Trains lightweight classifier on Snorkel labels
- Detects label-data mismatches via cross-validated probabilities
- Identifies and removes noisy labels
- Calculates overall label health score

### Research Citations
All methods are based on peer-reviewed research:
- **ISO/IEC/IEEE 29148:2018** - Requirements engineering quality
- **Cone of Uncertainty** (McConnell, 2006) - Estimation variance
- **Snorkel** (Ratner et al., 2017) - Data programming
- **Cleanlab** (Northcutt et al., 2021) - Confident learning

---

## ðŸ§ª Validation (Optional)

### Create Manual Validation Set
```python
import pandas as pd
from sklearn.model_selection import train_test_split

df = pd.read_csv('data/neodataset_augmented.csv')
sample, _ = train_test_split(df, train_size=100, stratify=df['risk_label'], random_state=42)
sample[['title', 'description', 'story_points', 'risk_label']].to_csv('data/manual_validation.csv', index=False)
```

Manually annotate 100 stories to calculate:
- **Precision**: % of RISK labels that are correct
- **Recall**: % of actual risks captured
- **F1 Score**: Harmonic mean of precision and recall

---

## ðŸ”„ Next Steps

### Option A: Use Augmented Data for Training
The augmented dataset is ready for training any ML classifier:
1. **TF-IDF + Logistic Regression** (recommended for PoC)
2. **BERT** or other transformer models (for production)
3. **Ensemble methods** (Random Forest, XGBoost)

### Option B: Refine Labeling Functions
If label health score < 0.60:
- Review LF statistics from Snorkel output
- Identify conflicting or low-coverage LFs
- Add domain-specific LFs based on your org's patterns
- Re-run augmentation pipeline

### Option C: Integrate into SprintGuard
Once you have a trained model:
1. Save model: `joblib.dump(model, 'models/risk_classifier.pkl')`
2. Create new `MLRiskAssessor` class implementing `RiskAssessorInterface`
3. Replace `KeywordRiskAssessor` in `app.py`
4. Test with real user stories

---

## ðŸ› Troubleshooting

**ImportError: No module named 'snorkel'**
```bash
pip install snorkel==0.9.9 cleanlab==2.6.0 datasets==2.14.0
```

**Low Label Health Score (<0.50)**
- Review LF analysis output to identify conflicts
- Consider adding more domain-specific LFs
- Check if risk definition matches your organizational context

**Memory Issues**
- NeoDataset has ~20K stories, requires ~4GB RAM
- For testing, use subset: modify `load_neodataset()` to return `df.head(1000)`

**Slow Download**
- HuggingFace datasets cached in `data/neodataset/`
- Subsequent runs will be faster

---

## ðŸ“ˆ Implementation Status

| Component | Status | Lines | Description |
|-----------|--------|-------|-------------|
| NeoDataset Loader | âœ… | 67 | Download & preprocess |
| Labeling Functions | âœ… | 348 | 18 research-backed LFs |
| Snorkel Pipeline | âœ… | 140 | Weak supervision |
| Cleanlab Pipeline | âœ… | 156 | Noise remediation |
| Exploration Script | âœ… | 49 | EDA tool |
| Augmentation Script | âœ… | 90 | Orchestrator |
| Documentation | âœ… | 450+ | Complete guide |

**Total Implementation:** 1,300+ lines of research-backed code

---

## ðŸŽ“ Learn More

- **Augmenting_NeoDataset.txt** - Complete research document with all citations
- **.plan.md** - Detailed implementation plan
- **docs/AUGMENTATION_STATUS.md** - Methodology details

---

## âœ¨ Summary

You now have a **complete, scientifically rigorous pipeline** for augmenting the NeoDataset with risk labels using weak supervision. This approach:

âœ… Requires **no manual labeling**  
âœ… Based on **peer-reviewed research**  
âœ… Generates **probabilistic labels** with confidence scores  
âœ… Includes **noise detection** and quality metrics  
âœ… Ready for **ML model training**  

Simply run `python scripts/augment_neodataset.py` to get started!
